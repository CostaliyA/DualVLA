<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="description" content="DualVLA: Building a Generalizable Embodied Agent via Partial Decoupling of Reasoning and Action">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DualVLA</title>

  <link rel="icon" href="static/images/logo4.png" type="image/png">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans:400,500,700|Noto+Sans:400,500" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css">

  <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']]
      },
      options: {
        ignoreHtmlClass: 'tex2jax_ignore',
        processHtmlClass: 'tex2jax_process'
      }
    };
  </script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <style>
    :root {
      --color-primary: #85acdc;
      --color-primary-dark: #7293bc;
      --color-text: #2c3e50;
      --color-bg: #FAFAFA;
      
      --color-think: #92c5ef;
      --color-gen: #b5dec8;
      --color-ref: #9C7BD8;
      --color-align: #7293bc;
    }

    html { scroll-behavior: smooth; }

    body {
      font-family: 'Google Sans', sans-serif;
      color: var(--color-text);
      background-color: var(--color-bg);
      line-height: 1.6;
      margin: 0;
    }

    .container {
      max-width: 1000px;
      margin: 0 auto;
      padding: 40px 20px;
    }

    /* --- Header Layout Fixed --- */
    .header-container { 
      text-align: center; 
      margin-bottom: 40px; 
    }

    .title {
      font-size: 2.5rem;
      line-height: 1.2;
      margin: 0 0 1.5rem 0;
      font-weight: 800;
    }

    /* Flex Container for Logo + Main Title */
    .main-title-row {
      display: flex;            /* Enable Flexbox */
      align-items: center;      /* Vertically center */
      justify-content: center;  /* Horizontally center */
      gap: 15px;                /* Space between logo and text */
      margin-bottom: 5px;
    }

    .project-logo {
      /* Removed absolute positioning that was hiding the image */
      width: auto;      /* Width determined by height */
      height: 1.5em;    /* Match text size approx */
      object-fit: contain;
    }
    
    .title-main { 
      color: #6591de; 
      font-style: italic; 
    }
    .title-sub { 
      color: #000000; 
      font-size: 0.75em; 
      display: block; 
      margin-top: 8px; 
    }

    .author-block { font-size: 1.15rem; margin-bottom: 8px; }
    .author-block a { color: #222; text-decoration: none; border-bottom: 1px dotted #888; transition: 0.2s; }
    .author-block a:hover { color: var(--color-primary-dark); }
    
    .affiliation-block { font-size: 1rem; color: #666; margin-bottom: 2rem; }
    .note { font-size: 0.85rem; color: #999; }

    /* Buttons */
    .publication-links {
      display: flex;
      justify-content: center;
      gap: 12px;
      flex-wrap: wrap;
    }
    .link-block a {
      display: inline-flex;
      align-items: center;
      background: #363636;
      color: #fff;
      padding: 10px 24px;
      border-radius: 99px;
      text-decoration: none;
      font-size: 1rem;
      font-weight: 500;
      transition: all 0.3s ease;
      box-shadow: 0 2px 5px rgba(0,0,0,0.1);
    }
    .link-block a:hover {
      background: var(--color-primary);
      transform: translateY(-2px);
      box-shadow: 0 5px 10px rgba(0,0,0,0.15);
    }
    .icon { margin-right: 8px; }

    /* Sections */
    .section {
      background: #fff;
      padding: 35px;
      border-radius: 16px;
      box-shadow: 0 4px 20px rgba(0,0,0,0.03);
      margin-bottom: 35px;
    }
    .section-title {
      text-align: center;
      font-size: 1.8rem;
      margin-bottom: 25px;
      position: relative;
      font-weight: 700;
      color: #444;
    }
    .section-title::after {
      content: ''; display: block; width: 50px; height: 4px;
      background: var(--color-primary); margin: 12px auto 0; border-radius: 2px;
    }

    .content-text {
      font-size: 1.1rem;
      text-align: justify;
      color: #4a4a4a;
    }
    .highlight-term { color: var(--color-primary-dark); font-weight: 700; }

    /* Images */
    .image-container { text-align: center; margin: 10px 0; }
    .image-container img {
      max-width: 100%;
      border-radius: 8px;
      box-shadow: 0 5px 15px rgba(0,0,0,0.05);
    }
    .video-grid {
      display: grid;
      grid-template-columns: repeat(2, 1fr);
      gap: 12px;
      margin: 10px 0;
    }

    .video-grid video {
      width: 100%;
      border-radius: 8px;
      box-shadow: 0 5px 15px rgba(0,0,0,0.05);
    }

    .teaser-img { width: 85%; } 
    
    .caption {
      font-size: 0.95rem;
      color: #666;
      margin-top: 15px;
      text-align: left;
      font-style: italic;
      max-width: 90%;
      margin: 15px auto 0 auto;
    }

    /* --- Method Grid --- */
    .method-grid {
      display: grid;
      grid-template-columns: repeat(4, 1fr); 
      gap: 20px;
      margin-top: 30px;
    }

    .method-card {
      background: #fff;
      border: 1px solid #eee;
      padding: 25px;
      border-radius: 12px;
      transition: transform 0.2s;
      display: flex;
      flex-direction: column;
      min-width: 0; 
    }
    .method-card:hover {
      transform: translateY(-3px);
      box-shadow: 0 10px 25px rgba(0,0,0,0.08);
    }
    
    .method-think { border-top: 5px solid var(--color-think); }
    .method-gen   { border-top: 5px solid var(--color-gen); }
    .method-ref   { border-top: 5px solid var(--color-ref); }
    .method-align   { border-top: 5px solid var(--color-align); }

    .method-card h3 { 
      margin-top: 0; 
      font-size: 1.25rem; 
      color: #333; 
      margin-bottom: 10px;
    }
    
    .method-card p {
      font-size: 0.95rem;
      color: #555;
      margin-bottom: 10px;
      flex-grow: 1; 
      word-wrap: break-word; 
    }

    .badge {
      display: inline-block; 
      padding: 4px 10px; 
      border-radius: 6px;
      font-size: 0.75rem; 
      font-weight: bold; 
      text-transform: uppercase; 
      letter-spacing: 0.5px;
      margin-bottom: 10px; 
      color: #fff;
      white-space: nowrap;
      width: fit-content;
    }

    /* Visualizations */
    .vis-card {
      margin-bottom: 40px;
      padding: 30px;
      border-radius: 16px;
      text-align: center;
      background: #fff;
      border: 1px solid rgba(0,0,0,0.04);
      box-shadow: 0 4px 20px rgba(0,0,0,0.04);
      transition: transform 0.3s;
    }
    .vis-card:hover { transform: scale(1.005); box-shadow: 0 8px 30px rgba(0,0,0,0.08); }

    .vis-header {
      display: inline-block;
      font-size: 1.2rem;
      font-weight: 700;
      margin-bottom: 15px;
      padding: 6px 18px;
      border-radius: 50px;
    }

    /* BibTeX */
    pre {
      background-color: #2d2d2d;
      color: #ccc;
      padding: 20px;
      border-radius: 10px;
      font-size: 0.85rem;
      overflow-x: auto;
      line-height: 1.5;
      border: 1px solid #444;
    }

    @media (max-width: 768px) {
      .main-title-row { flex-direction: column; } /* Stack logo on mobile */
      .title { font-size: 2rem; }
      .teaser-img { width: 90%; }
      .container { padding: 20px 15px; }
      .method-grid { grid-template-columns: 1fr; }
    }
  </style>
</head>
<body>

<div class="container">

  <div class="header-container">
    <h1 class="title">
        <div class="main-title-row">
          <img src="static/images/logo4.png" alt="Logo" class="project-logo" style="height: 1.5em; width: auto;">
          <span class="title-main">DualVLA:</span>
        </div>
        <span class="title-sub">Building a Generalizable Embodied Agent via Partial Decoupling of Reasoning and Action</span>
    </h1>
    
    <div class="author-block">

                 <a href="https://costaliya.github.io//">Zhen Fang<sup>1*</sup></a>,
                 <a href="https://zhuoyangliu2005.github.io/">Zhuoyang Liu<sup>2*</sup></a>,
                <a href="https://liujiaming1996.github.io/">Jiaming Liu<sup>2†</sup></a>,
                <a href="https://chen-h01.github.io/">Hao Chen<sup>3</sup></a>,
                Yu Zeng<sup>1</sup>,<br>Shiting Huang<sup>1</sup>,
                <a href="https://lovesnowbest.site/">Zehui Chen<sup>1†</sup></a>, 
                <a href="https://siyuyuan.github.io/">Lin Chen<sup>1</sup></a>,
                <a href="https://www.shanghangzhang.com/">Shanghang Zhang<sup>2#</sup></a>,
                <a href="https://scholar.google.co.uk/citations?user=r6CvuOUAAAAJ&hl=en">Feng Zhao<sup>1#</sup></a>,



    </div>


    <div class="affiliation-block">
      <div><sup>1</sup>MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition, USTC &nbsp;&nbsp; <br>
        <sup>2</sup>State Key Laboratory of Multimedia Information Processing, School of Computer Science, Peking University &nbsp; <sup>3</sup>CUHK</div>
      <div class="note"><sup>*</sup>Equal Contribution &nbsp; <sup>†</sup>Project Lead &nbsp; <sup>#</sup>Corresponding Authors</div>
    </div>

    <div class="publication-links">
      <span class="link-block">
        <a href="https://arxiv.org/abs/2511.22134" target="_blank">
          <span class="icon"><i class="fas fa-file-pdf"></i></span>
          <span>Paper</span>
        </a>
      </span>
      <span class="link-block">
        <a href="https://costaliya.github.io/DualVLA/" target="_blank">
          <span class="icon"><i class="fab fa-github"></i></span>
          <span>Code</span>
        </a>
      </span>
      <span class="link-block">
        <a href="https://costaliya.github.io/DualVLA/" target="_blank">
          <span class="icon"><i class="fas fa-database"></i></span>
          <span>Data</span>
        </a>
      </span>
          <span class="link-block">
        <a href="https://costaliya.github.io/DualVLA/" target="_blank">
          <span class="icon"><i class="fas fa-robot"></i></span>
          <span>Model</span>
        </a>
      </span>
    </div>
  </div>

  <!-- <div class="section">
    <div class="image-container">
      <img src="figure/teaser.jpg" alt="Teaser Image" class="teaser-img">
    </div>
    <div class="caption">

<b>DualVLA</b> first constructs a sparse, information-dense embodied reasoning dataset by combining video event prediction with kinematic cues, mitigating the negative impact of redundant reasoning on action generation. It then adopts a dual-teacher strategy: an action teacher offering fine-grained supervision for manipulation, and a reasoning teacher maintaining general reasoning capability. Together, these components enable <b>DualVLA</b> to achieve strong performance in both simulation and real-world robotic evaluations.


    </div>
  </div> -->

  <div class="section">
    <h2 class="section-title">Abstract</h2>
    <div class="content-text">
      <p>
      To build a generalizable Vision–Language–Action (VLA) model with reasoning capability, a common approach is to first train a specialist VLA on robot demonstrations to acquire reliable manipulation skills, and then introduce mixed annotated-robot data with multimodal data to restore general reasoning. However, we observe that the resulting reasoning VLA exhibits degraded action performance compared to the specialist VLA before fine-tuning. We define this phenomenon as <b>Action Degeneration</b>. 
      </p>
      <p>
      To tackle this issue, we propose <b>DualVLA</b>, which improves action performance through carefully designed post-training while preserving the reasoning ability. We first propose a <b>dual-layer data pruning</b> method to remove redundant embodied reasoning and alleviate its adverse guidance on action learning. To further enhance the model's action generation capabilities, we adopt a <b>dual-teacher adaptive distillation</b> strategy that assigns different supervision signals to different data domains and maintains its reasoning ability. 
      </p>
      <p>
      To fill the evaluation gap of generalist VLAs, we introduce <b>VLA Score</b>, which decouples VLA capabilities into reasoning, intention, action, and alignment, enabling a more fine-grained evaluation. Experiments show that <b>DualVLA</b> achieves an average success rate of <b>61.0</b> in SimplerEnv and an average score of <b>65.4</b> across eight competitive multimodal benchmarks, demonstrating a stronger balance between action execution and multimodal understanding.
      </p>
    </div>
  </div>


  <div class="section">
    <h2 class="section-title">Framework</h2>
    
    <div class="image-container">
      <img src="static/images/teaser.jpg" alt="DualVLA Framework">
    </div>
    <div class="caption">

  <p style="font-size:1rem;line-height:1.6;color:#333;">
    <strong>The overview of <b>DualVLA.</b></strong> <b>DualVLA</b> introduces a sparse yet information-dense embodied reasoning dataset by integrating video event prediction with kinematic cues, selectively retaining reasoning segments that are truly relevant to action execution and suppressing redundant or low-value descriptions that typically hinder policy learning. Building on this foundation, <b>DualVLA</b> adopts a dual-teacher distillation strategy: an action teacher that delivers fine-grained manipulation guidance for precise control, and a reasoning teacher that maintains broad multimodal reasoning ability without overfitting to specific tasks. By jointly enhancing actionable skills and preserving general reasoning, <b>DualVLA</b> demonstrates strong adaptability and performance across both simulation benchmarks and real-world robotic evaluations, offering a scalable paradigm for robust vision-language-action models.
  </p>
    </div>

  </div>


  <div class="section">
    <h2 class="section-title">Evaluation: VLA Score</h2>
    
    <div class="image-container">
      <img src="static/images/eval.png" alt="VLA Score Framework">
    </div>
    <div class="caption">

  <p style="font-size:1rem;line-height:1.6;color:#333;">
<p>
<b>VLA Score</b> evaluation pipeline overview: Given the policy trajectory, task description, and optional reasoning as input, <b>VLA Score</b> first performs dual retrieval to fetch task-relevant textual examples and visually similar trajectories from a curated knowledge base. The retrieved samples serve as few-shot context for the VLM judge, which evaluates the trajectory along four dimensions: Reasoning, Action, Intention, and Alignment. These scores are then combined with the simulation outcome to produce the final <b>VLA Score</b>.
</p>

  </p>
    </div>

    <div class="method-grid">
        <div class="method-card method-think">
          <span class="badge" style="background: var(--color-think); color: #555;">Reasoning</span>
          <h3>$R$: How to Think</h3>
          <p>
           Measures the correctness, logical consistency, and usefulness of the reasoning process.
          </p>
        </div>
    
        <div class="method-card method-gen">
          <span class="badge" style="background: var(--color-gen);">Action</span>
          <h3>$A$: How to Act</h3>
          <p>
          Measures the coherence and smoothness of the action sequence.
          </p>
        </div>
    
        <div class="method-card method-ref">
          <span class="badge" style="background: var(--color-ref);">Intension</span>
          <h3>$I$: How to Try</h3>
          <p>
              Determines whether the model’s actions contribute constructively to solving the task.
          </p>
        </div>
        <div class="method-card method-align">
          <span class="badge" style="background: var(--color-align);">Alignment</span>
          <h3>$RA$: How to Align</h3>
          <p>
            Measures how well the action sequences align with reasoning content.
          </p>
        </div>
      </div>
      <br>
          <div class="caption">
Combined with the trajectory’s simulation result $B$, we calculate the overall VLAs Score using the following formula:
$$
{\small
\begin{equation}
\text{VLA Score }  =
\begin{cases}
\left( \dfrac{R + A \cdot I}{2} \right) \cdot RA \cdot B, & \pi_\theta \in \text{RVLA} \\[4pt]
A \cdot I \cdot RA \cdot B, & \pi_\theta \notin \text{RVLA}
\end{cases}
\end{equation}
,
\text{where } B = 
\begin{cases}
1, & \text{if success} \\
0, & \text{if fall}
\end{cases}

}
$$</div>

</div>
  <div class="section" id="visuals" style="background: transparent; box-shadow: none; padding: 0;">
    <h2 class="section-title">Results</h2>

    <div class="vis-card">
      <div class="vis-header" style="background: #FFF3E0; color: #E67E22;">
        <i class="fas fa-gamepad"></i> SimplerEnv Results
      </div>
      <div class="image-container">
        <img src="static/images/simplerenv.png" alt="simplerenv">
      </div>
      <div class="caption">
      <strong>Comparison of manipulation success rates between <b>DualVLA</b> and specialist & generalist baselines in SimplerEnv.</strong>
      Google Robot and WidowX Robot denote two embodiments in SimplerEnv. VM refers to visual matching and VA refers to variance aggregation. 
      $^{\dagger}$ denotes models without released checkpoints, results are taken from their papers.
      </div>
    </div>
    <div class="vis-card">
      <div class="vis-header" style="background: #F3E5F5; color: #8E24AA;">
        <i class="fas fa-robot"></i> Real-World Tasks
      </div>
      <div class="image-container">
        <img src="static/images/real_task_1.png" alt="real">
      </div>
      <div class="caption">
       To systematically evaluate our approach, we designed two real-world dual-arm tasks on the Galaxea R1-lite robot. For the dual-arm setting, three RealSense 455 camera are used to get image observations, one on the head and two on the left and right wrist, respectively. The model takes the images of three views as image observation, and outputs a 14-DoF vector as the dual-arm action.
      We designe two complex tasks:(1) Move Objects, (2) Handover Objects. Both tasks require the model to move three objects from right to left and follow the order in the language instruction. For each task, we collected 50 high-quality demonstration trajectories. 
      We test 10 rollouts for each task and use the average success rate as the quantitative result. Result shows that <b>DualVLA</b> significantly improves manipulation performance, raising the average success rate from <b>45.0% to 60.0%</b> in real-world tasks.
      The gains in both Move and Handover tasks demonstrate more reliable and coordinated action generation in real robotic settings.
      </div>
    </div>
        <div class="vis-card">
      <div class="vis-header" style="background: #E1F5FE; color: #0288D1;">
        <i class="fas fa-flask"></i> VLA Score Results
      </div>
      <div class="image-container">
        <img src="static/images/vla_score_result.png" alt="vla_score_result">
      </div>
      <div class="caption">
  <p style="font-size:1rem;line-height:1.6;color:#333;">
    Our model, <b>DualVLA</b>, achieves the highest score in <b>VLA Score</b> among reasoning VLAs. Reasoning VLAs display a substantially higher reasoning score than both their action and alignment scores. Failure case analysis shows that their low action scores mainly stem from the inability to produce effective execution throughout the trajectory: although these models often reason correctly about how a task should be completed, they still struggle to approach or manipulate the target properly. <b>DualVLA</b> inherits strong reasoning capability from the reasoning teacher while learning refined and smooth action behaviors from the action teacher—capabilities that cross-entropy training alone cannot provide. We argue that this combination is a critical factor contributing to the effectiveness of <b>DualVLA</b>.
  </p>
      </div>
    </div>

  </div>
  <div class="section" id="visuals" style="background: transparent; box-shadow: none; padding: 0;">
    <h2 class="section-title">Visualizations</h2>

    <div class="vis-card">
      <div class="vis-header" style="background: #FFF3E0; color: #E67E22;">
        <i class="fas fa-gamepad"></i> SimplerEnv
      </div>
      <div class="image-container">
        <img src="static/images/simplerenv_google.png" alt="google">
      </div>
      <div class="image-container">
        <img src="static/images/simplerenv_widowx.png" alt="WidowX">
      </div>
      <div class="caption">
          Visual examples of SimplerEnv Google robot tasks driven by <b>DualVLA</b> (top) and WidowX robot tasks (bottom). Compared to the generalist baseline, <b>DualVLA</b> generates more accurate and smoother action sequences that better align with the reasoning process, leading to successful task completion.
      </div>
    </div>

    <div class="vis-card">
      <div class="vis-header" style="background: #F3E5F5; color: #8E24AA;">
        <i class="fas fa-robot"></i> Real-World Tasks
      </div>
      <div class="video-grid">
          <video src="static/images/demo1.mp4" controls></video>
          <video src="static/images/demo2.mp4" controls></video>
          <video src="static/images/demo3.mp4" controls></video>
          <video src="static/images/demo4.mp4" controls></video>
      </div>
      <div class="caption">
            Demonstration videos of two real-world manipulation tasks: the top row shows <b>moving objects</b>, and the bottom row shows <b>handover object manipulation</b>.

      </div>
    </div>

    <div class="vis-card">
      <div class="vis-header" style="background: #E1F5FE; color: #0288D1;">
        <i class="fas fa-brain"></i> Multimodal Reasoning
      </div>
      <div class="image-container">
      <img src="static/images/mm.png" alt="mm reasoning" style="width: 35%;">

      </div>
      <div class="caption">
        Distillation from the reasoning teacher effectively preserves multimodal reasoning capabilities, with no significant difference compared to the reasoning teacher or the base VLM.
      </div>
    </div>

  </div>

  <div class="section">
    <h2 class="section-title">BibTeX</h2>
    <pre><code>@article{fang2026dual,
  title={DualVLA: Building a Generalizable Embodied Agent via Partial Decoupling of Reasoning and Action},
  author={Zhen Fang and Zhuoyang Liu and Jiaming Liu and Hao Chen and Yu Zeng and Shiting Huang and Zehui Chen and Lin Chen and Shanghang Zhang and Feng Zhao},
  journal={arXiv:2511.22134},
  year={2025}
}</code></pre>
  </div>

  <footer style="text-align: center; padding: 30px 0; color: #999; font-size: 0.9rem;">
    <p>
    <span style="font-size: 0.8rem;">This website is adapted from <a href="https://think-while-gen.github.io/" style="color: #999;">TwiG</a> and <a href="https://github.com/nerfies/nerfies.github.io" style="color: #999;">Nerfies</a>. <br>Thank you for their contributions to the community.</span>
    </p>
  </footer>

</div>

</body>
</html>
